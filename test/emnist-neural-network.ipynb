{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/adam/Workspace/personal/unihack/fast-form'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir(\"../\")\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_cell_guid": "f2e9cf0b-4025-4de3-b57b-bae842beef1c",
    "_uuid": "4e52b01454a737738ed560d4e25bf770572d57e2"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "train_db = pd.read_csv(\"training_data/emnist-balanced-train.csv\")\n",
    "test_db  = pd.read_csv(\"training_data/emnist-balanced-test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_cell_guid": "db92487d-656a-4cec-958a-0c0c2864a2e6",
    "_uuid": "6124d2bf78a6b9b237cd4e25c0ae3986f060abbd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_train: (112799, 47)\n",
      "x_train: (112799, 784)\n",
      "WARNING:tensorflow:From /home/adam/.local/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:20: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"de...)`\n",
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:28: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/adam/.local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 101519 samples, validate on 11280 samples\n",
      "Epoch 1/10\n",
      "101519/101519 [==============================] - 9s 93us/step - loss: 1.2407 - accuracy: 0.6589 - val_loss: 0.8499 - val_accuracy: 0.7462\n",
      "Epoch 2/10\n",
      "101519/101519 [==============================] - 10s 96us/step - loss: 0.7157 - accuracy: 0.7868 - val_loss: 0.6585 - val_accuracy: 0.7908\n",
      "Epoch 3/10\n",
      "101519/101519 [==============================] - 8s 83us/step - loss: 0.5633 - accuracy: 0.8247 - val_loss: 0.5617 - val_accuracy: 0.8179\n",
      "Epoch 4/10\n",
      "101519/101519 [==============================] - 10s 100us/step - loss: 0.4867 - accuracy: 0.8445 - val_loss: 0.5280 - val_accuracy: 0.8251\n",
      "Epoch 5/10\n",
      "101519/101519 [==============================] - 8s 80us/step - loss: 0.4345 - accuracy: 0.8568 - val_loss: 0.5101 - val_accuracy: 0.8316\n",
      "Epoch 6/10\n",
      "101519/101519 [==============================] - 8s 80us/step - loss: 0.3969 - accuracy: 0.8672 - val_loss: 0.4897 - val_accuracy: 0.8326\n",
      "Epoch 7/10\n",
      "101519/101519 [==============================] - 9s 84us/step - loss: 0.3658 - accuracy: 0.8764 - val_loss: 0.4782 - val_accuracy: 0.8392\n",
      "Epoch 8/10\n",
      "101519/101519 [==============================] - 8s 82us/step - loss: 0.3416 - accuracy: 0.8821 - val_loss: 0.4737 - val_accuracy: 0.8395\n",
      "Epoch 9/10\n",
      "101519/101519 [==============================] - 8s 83us/step - loss: 0.3169 - accuracy: 0.8913 - val_loss: 0.4677 - val_accuracy: 0.8438\n",
      "Epoch 10/10\n",
      "101519/101519 [==============================] - 8s 83us/step - loss: 0.2986 - accuracy: 0.8959 - val_loss: 0.4635 - val_accuracy: 0.8447\n",
      "y_test: (18799, 47)\n",
      "x_test: (112799, 784)\n",
      "18799/18799 [==============================] - 2s 92us/step\n",
      "[0.47837314857016905, 0.8486089706420898]\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import np_utils\n",
    "from keras.layers import Input, Dense, Dropout\n",
    "from keras.models import Model\n",
    "from keras.models import model_from_json\n",
    "\n",
    "num_classes = 47\n",
    "y_train = train_db.iloc[:,0]\n",
    "y_train = np_utils.to_categorical(y_train, num_classes)\n",
    "print (\"y_train:\", y_train.shape)\n",
    "\n",
    "x_train = train_db.iloc[:,1:]\n",
    "x_train = x_train.astype('float32')\n",
    "x_train /= 255\n",
    "print (\"x_train:\",x_train.shape)\n",
    "\n",
    "inp = Input(shape=(784,))\n",
    "hidden_1 = Dense(1024, activation='relu')(inp)\n",
    "dropout_1 = Dropout(0.2)(hidden_1)\n",
    "out = Dense(num_classes, activation='softmax')(hidden_1) \n",
    "model = Model(input=inp, output=out)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', # using the cross-entropy loss function\n",
    "              optimizer='adam', # using the Adam optimiser\n",
    "              metrics=['accuracy']) # reporting the accuracy\n",
    "\n",
    "model.fit(x_train, y_train, # Train the model using the training set...\n",
    "          batch_size=512, nb_epoch=10,\n",
    "          verbose=1, validation_split=0.1) # ...holding out 10% of the data for validation\n",
    "\n",
    "y_test = test_db.iloc[:,0]\n",
    "y_test = np_utils.to_categorical(y_test, num_classes)\n",
    "print (\"y_test:\", y_test.shape)\n",
    "\n",
    "x_test = test_db.iloc[:,1:]\n",
    "x_test = x_test.astype('float32')\n",
    "x_test /= 255\n",
    "print (\"x_test:\",x_train.shape)\n",
    "\n",
    "print(model.evaluate(x_test, y_test, verbose=1)) # Evaluate the trained model on the test set!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 84.86%\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model save/load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 84.86%\n",
      "Saved model to disk\n",
      "Loaded model from disk\n",
      "accuracy: 84.86%\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    " \n",
    "# serialize model to JSON\n",
    "model_json = model.to_json()\n",
    "with open(\"model_data/model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"model_data/model.h5\")\n",
    "print(\"Saved model to disk\")\n",
    " \n",
    "# later...\n",
    " \n",
    "# load json and create model\n",
    "json_file = open('model_data/model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"model_data/model.h5\")\n",
    "print(\"Loaded model from disk\")\n",
    " \n",
    "# evaluate loaded model on test data\n",
    "loaded_model.compile(loss='categorical_crossentropy', # using the cross-entropy loss function\n",
    "              optimizer='adam', # using the Adam optimiser\n",
    "              metrics=['accuracy']) # reporting the accuracy\n",
    "\n",
    "\n",
    "scores = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"%s: %.2f%%\" % (loaded_model.metrics_names[1], scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import PIL.Image\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '0', 1: '1', 2: '2', 3: '3', 4: '4', 5: '5', 6: '6', 7: '7', 8: '8', 9: '9', 10: 'A', 11: 'B', 12: 'C', 13: 'D', 14: 'E', 15: 'F', 16: 'G', 17: 'H', 18: 'I', 19: 'J', 20: 'K', 21: 'L', 22: 'M', 23: 'N', 24: 'O', 25: 'P', 26: 'Q', 27: 'R', 28: 'S', 29: 'T', 30: 'U', 31: 'V', 32: 'W', 33: 'X', 34: 'Y', 35: 'Z', 36: 'a', 37: 'b', 38: 'd', 39: 'e', 40: 'f', 41: 'g', 42: 'h', 43: 'n', 44: 'q', 45: 'r', 46: 't'}\n"
     ]
    }
   ],
   "source": [
    "with open(\"training_data/emnist-balanced-mapping.txt\") as f:\n",
    "    s = f.read()[:-1] # remove last char \\n\n",
    "a = [l.split(\" \") for l in s.split(\"\\n\")]\n",
    "res_mapper = {int(l[0]): chr(int(l[1])) for l in a}\n",
    "print(res_mapper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Display data from dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.int64"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(test_db.iloc[i].values[1:][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_letter_from_db(img):\n",
    "    img = img.astype('uint8')\n",
    "    img = img.reshape((28,28))\n",
    "    img = img.transpose()\n",
    "    img = cv2.bitwise_not(img)\n",
    "    \n",
    "    display(PIL.Image.fromarray(img))\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "def revert_showing(img):\n",
    "    img = cv2.bitwise_not(img)\n",
    "    img = img.transpose()\n",
    "    img = img.ravel()\n",
    "    img = img.astype('int64') \n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_char(i):\n",
    "    label = test_db.iloc[i].values[0]\n",
    "    char = res_mapper.get(label)\n",
    "    print(\"=============\")\n",
    "    print(f\"label: {char}\")\n",
    "    \n",
    "    img = test_db.iloc[i].values[1:]\n",
    "    print(\"showing original\")\n",
    "    img_new=show_letter_from_db(img)\n",
    "    \n",
    "    img = revert_showing(img_new)\n",
    "    \n",
    "    pred = model.predict(np.array([img]))\n",
    "    pred_val = res_mapper.get(pred[0].argmax())\n",
    "    print(f\"Prediction: {pred_val}\")\n",
    "    print(\"showing the one prediction run on\")\n",
    "    show_letter_from_db(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============\n",
      "label: e\n",
      "showing original\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABwUlEQVR4nJ2ST2sTURTFz3kzr5M0k3Gq1FEXppUYxCzEpSh0F+guIO4i6Er0Q0g3ulEQP4Fd1LXtsgtBUCnZ1BKJlKhJ3QgG46ikOslk5rrI36px4dk9ftz77jn3UjBd6h8M5oGXtHxoTwNK/QGlfmcL9lIaTrZgAYCM1XtbmiWV1jp9ficWkTHsvV8vJplYzOUWE0yU6iJijr97/myrwxNFW31f3+uWKyeNUdtg55qXIBce+fs//AeuMq80h5USPLm32wF4ePchEG//lKj6bb5fGb9byyoSoKG11togjcvNwUD1UlaRJAGSJLV27of9tlGl/FGgKDEBKAXtaTenABOQxmoj4vzFzIeXvus6mbxKL9nW8T5s3d2MsHB72W6/eHM2b6VcjnITqZ2hSq3si0gURjIpE0CMIxeuzmIY9+8rs0950/fZrn+aAvUhtp6uBX+jIsGqR557FfbCMOw2a7VaazgQBWje3IitwqWjn9uIq9XYvHFrbEW6KzOkkZxL9mM1r/sTVujM9CQOOoNDNEeGTABmcbvciCBKAZw7Vlh2BpACQPYqj1/HTiavoPLZ05aahED05St0yuXBlPjfF/8Lre7kNuj321YAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7FCF2B2B6ED0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: e\n",
      "showing the one prediction run on\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABwUlEQVR4nJ2ST2sTURTFz3kzr5M0k3Gq1FEXppUYxCzEpSh0F+guIO4i6Er0Q0g3ulEQP4Fd1LXtsgtBUCnZ1BKJlKhJ3QgG46ikOslk5rrI36px4dk9ftz77jn3UjBd6h8M5oGXtHxoTwNK/QGlfmcL9lIaTrZgAYCM1XtbmiWV1jp9ficWkTHsvV8vJplYzOUWE0yU6iJijr97/myrwxNFW31f3+uWKyeNUdtg55qXIBce+fs//AeuMq80h5USPLm32wF4ePchEG//lKj6bb5fGb9byyoSoKG11togjcvNwUD1UlaRJAGSJLV27of9tlGl/FGgKDEBKAXtaTenABOQxmoj4vzFzIeXvus6mbxKL9nW8T5s3d2MsHB72W6/eHM2b6VcjnITqZ2hSq3si0gURjIpE0CMIxeuzmIY9+8rs0950/fZrn+aAvUhtp6uBX+jIsGqR557FfbCMOw2a7VaazgQBWje3IitwqWjn9uIq9XYvHFrbEW6KzOkkZxL9mM1r/sTVujM9CQOOoNDNEeGTABmcbvciCBKAZw7Vlh2BpACQPYqj1/HTiavoPLZ05aahED05St0yuXBlPjfF/8Lre7kNuj321YAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7FCF2B428250>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============\n",
      "label: 9\n",
      "showing original\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABcklEQVR4nHVSz0sCURB+o2/XZZWgQCUIScWti0jQ0SShOoXQQQgSOncLwruBt/wzugQFEkEd7WaQEUVSotglwiwjSER2900Hn7k/9Du9mW9+ffMGkEyGw+ZhGhs+qYnQquWfm6foXpLbOILeKMSmZAFcWe4Ykd1i2udWlN0jLyyqFlLLh4VY7rn2/hixke28JGe6iNpGUBQOLGVPZ6WdOiKqCoCQ404upX/cXs6GCCEdlRiUD2IqglBFRFZcAwD/Gc8c6NRKGv1VQa1lm4SQlbgpUz+XwasoQdmdWnUKt2geqJfxOB0O0X9Y36dibUgCb99/u2/C5vTM9fZXtCxZdusKzTNCSe/qk65LlmmHKIUgXEFLTw41KdL0ByIbR7Z8kGogsldm3BCf6rJDEgFCIAD2ni8LEG4ZbFPmwzfd8k64If2kE0iAMdpQpSFDQcXxZfW7Pk2aD+4/TL+IiPGeSZkh0rM0l5dMiWD4d4ZOMpG04Q+rH2hm7QwoxgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7FCF2B2B6ED0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: 9\n",
      "showing the one prediction run on\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABcklEQVR4nHVSz0sCURB+o2/XZZWgQCUIScWti0jQ0SShOoXQQQgSOncLwruBt/wzugQFEkEd7WaQEUVSotglwiwjSER2900Hn7k/9Du9mW9+ffMGkEyGw+ZhGhs+qYnQquWfm6foXpLbOILeKMSmZAFcWe4Ykd1i2udWlN0jLyyqFlLLh4VY7rn2/hixke28JGe6iNpGUBQOLGVPZ6WdOiKqCoCQ404upX/cXs6GCCEdlRiUD2IqglBFRFZcAwD/Gc8c6NRKGv1VQa1lm4SQlbgpUz+XwasoQdmdWnUKt2geqJfxOB0O0X9Y36dibUgCb99/u2/C5vTM9fZXtCxZdusKzTNCSe/qk65LlmmHKIUgXEFLTw41KdL0ByIbR7Z8kGogsldm3BCf6rJDEgFCIAD2ni8LEG4ZbFPmwzfd8k64If2kE0iAMdpQpSFDQcXxZfW7Pk2aD+4/TL+IiPGeSZkh0rM0l5dMiWD4d4ZOMpG04Q+rH2hm7QwoxgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7FCF2B1F6690>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============\n",
      "label: Q\n",
      "showing original\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABz0lEQVR4nHWSP0gbcRTH311iLtGeTlrJ0JIqJSLBf4NLLSJI7eCfxcGmQxzETRwEcbOuigriIIJk6NASQZ3aQQlKQagIsU1jCSlSLYRTE9RAvOvlfl+Hu9OfSt703vu893j/BFBxcfIGY/c9nGr8jf1iJPd53bZHsMoi8y81e0VE5Ol+/9xxD+qJhe9lr1vrRbp8p3QMvpGsFADA565wKj53BABKpKN60gAAmJCt/VSVUMWsDgDavNercBAF5MNPSyZ1AMDZcnVY5SAK67VS3YIJ8ael8YCD6scXnpBm2JER95QOQDTb+j1z4h91WQY5AiIjIjJtbTrZNuHO2sMjZw5owvQGvQ2MLKkWzCwy8Q5mWXPn4cFmxmRafOdZo2gvIT/2MsH2a1YKAKDGgk/kGAMAJxGREiWnIDsS/z1E2qf5pDzkF26voueIiCia9hnZL+MXr7oHJO5konZhgI63mrZ3v1HPcJvLbhvAab+7N/LVJ1b6Sl01q4q9C2tDP+qqpBJBoPLQqoI7Mcv61+O70Zye1tSGKv5vrCAjf5TcC5aWfbjmMonTWSroaYgVgYDSL9WmWBFobPbJQ+cPGrJFbPdtSMKtKTz8+AL3yo8gLzfYL2isS72EWAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7FCFBF898E10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Q\n",
      "showing the one prediction run on\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABz0lEQVR4nHWSP0gbcRTH311iLtGeTlrJ0JIqJSLBf4NLLSJI7eCfxcGmQxzETRwEcbOuigriIIJk6NASQZ3aQQlKQagIsU1jCSlSLYRTE9RAvOvlfl+Hu9OfSt703vu893j/BFBxcfIGY/c9nGr8jf1iJPd53bZHsMoi8y81e0VE5Ol+/9xxD+qJhe9lr1vrRbp8p3QMvpGsFADA565wKj53BABKpKN60gAAmJCt/VSVUMWsDgDavNercBAF5MNPSyZ1AMDZcnVY5SAK67VS3YIJ8ael8YCD6scXnpBm2JER95QOQDTb+j1z4h91WQY5AiIjIjJtbTrZNuHO2sMjZw5owvQGvQ2MLKkWzCwy8Q5mWXPn4cFmxmRafOdZo2gvIT/2MsH2a1YKAKDGgk/kGAMAJxGREiWnIDsS/z1E2qf5pDzkF26voueIiCia9hnZL+MXr7oHJO5konZhgI63mrZ3v1HPcJvLbhvAab+7N/LVJ1b6Sl01q4q9C2tDP+qqpBJBoPLQqoI7Mcv61+O70Zye1tSGKv5vrCAjf5TcC5aWfbjmMonTWSroaYgVgYDSL9WmWBFobPbJQ+cPGrJFbPdtSMKtKTz8+AL3yo8gLzfYL2isS72EWAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7FCF2B1F6690>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in range (3):\n",
    "    display_char(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "# with open(\"test.txt\", \"wb\") as fp:   #Pickling\n",
    "#     pickle.dump(l, fp)\n",
    "\n",
    "with open(\"./test/example_forms/julinka_dotaznik/example_letters.pickle\", \"rb\") as fp:   # Unpickling\n",
    "        b = pickle.load(fp)\n",
    "\n",
    "len(b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAtklEQVR4nGP4jwoYTJA4TAwoQJ/xIRIPTfLd/9fIXDRTGVgO4TSWIeDPWlw6ZzN8Y9BFMgdFMoLhPwMzdslHQhDDArFJijAwMjEjW4VqbOLB/xsZ/s/AKlmh+P//EiQRZMmLDPX//79jWI1VUonh6f///5F8hyzJYPgfj+QV3JJQ0TaGZ5iSfxk4IQzGFzAhFkRUMnyDMP7BhRj/w1kMcCZmZKdcQJdD0okFoEf2kJCMRxFMgdIAVPZdT9PpM5QAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7FCF2B208E50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "img = b[1]\n",
    "display(PIL.Image.fromarray(img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preproces to training format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_char(img):\n",
    "#     label = test_db.iloc[i].values[0]\n",
    "#     char = res_mapper.get(label)ma\n",
    "#     print(f\"label: {char}\")\n",
    "    \n",
    "    \n",
    "    pred = model.predict(np.array([img]))\n",
    "    pred_val = res_mapper.get(pred[0].argmax())\n",
    "    print(f\"Prediction: {pred_val}\")\n",
    "    \n",
    "    \n",
    "    img = img.astype('uint8') \n",
    "    img = img.reshape((28,28))\n",
    "    img = img.transpose()\n",
    "    display(PIL.Image.fromarray(img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============\n",
      "Prediction: F\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAvElEQVR4nLWQsQrCMBRFT2rBDhXRRYSCi6Af4N+5qZ8gOPkBzq46iruDBTuIo4KLQxWsjYOUxkje5p1yc3IevCgAgjUA5IMAQH0aPgB3iryU99wXRQEkNxVeRguv1yDGTgJQ0Y8fUOas+07m53PB1Lppvy+PWepJZk2CgjmMJ9aN8Tjq24sa8HaaumHYaQvm9mhBY89ZXfjclh66x16XXbeJ1pHTZEzVbUZ6Jc09CDBLv7tvls1TMP+fHcAbUywxmm4DBUsAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7FCFBF898E10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============\n",
      "Prediction: P\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAoElEQVR4nGNgQAX/TzPgBBf/vcIt+fj/f9yS////tkXwmNBkN7AE49KY8p/z/yVcksv/M/z/g11K9u3///////+/Dpvk6////v75/x+Xg+fZMfj9Z0jHKtd+j4EhGoc+vf8NDAyC/0OwSt79L8XAwIDDyv/nGPBIauOWhIpW/pfElGP6/w3C+CeOKXkZT2ThcggEzNbHIzlcwEIU3hwoDQDALkLKSe63GgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7FCF2B1E7A90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============\n",
      "Prediction: P\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAu0lEQVR4nOXOvQqBYRjG8b+vLE5AyehjMDOZJDM5CqWMdhajI3AmymKQiRKTRVEWpXy/l+Gh9x1uR+Barqf711UPuCSeknTFjHYDSfKGNnNRsSWpYWLayxKe6VW1MOcVgLE2Fq5VAlg9K99L2Melq1NEBkZc1fxLADuuMibGXfVN/CTGxPqumgDy7GUdSDOyhugMUck/+MsQN+DB3B7e9yyCQ0IBpJtKlvNHc9mWthVTAHo6/LS/y/T7eAPvAz/bOSXMoAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7FCF2B1D7050>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============\n",
      "Prediction: q\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAvklEQVR4nK2QIQ4CMRBFXxsMBoPAoTgBggNwAjQGTxBYEtReYB0WiyccAQkEQ0JCyApwSMAtg9hs0t12NgjGtPlv/p9pAQCJe6g1fm0AUlm7qs2Oxa0xAbY8g14RYCcrBU7hIQXN5pcRbUeOPWvEPXf2PZhmgwOxmLddBrcBoCsiM8XJ+Qgn1dqSj1VhXS5FwW0dlLtd2CHRoWGujmQvOiv/TzHWKwc2q/r82JpzH15N1Up/qAhQn5L8mnIIy18o6jWTFGf6BgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7FCF2B1D7290>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected input_1 to have shape (784,) but got array with shape (1,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-295-9bf791d7a3f3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mdisplay_char\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrevert_showing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-290-ff8066f1b798>\u001b[0m in \u001b[0;36mdisplay_char\u001b[0;34m(img)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mpred_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres_mapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Prediction: {pred_val}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1439\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1440\u001b[0m         \u001b[0;31m# Case 2: Symbolic tensors or Numpy array-like.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1441\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_standardize_user_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1442\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstateful\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1443\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    577\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m             exception_prefix='input')\n\u001b[0m\u001b[1;32m    580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    143\u001b[0m                             \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m                             str(data_shape))\n\u001b[0m\u001b[1;32m    146\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking input: expected input_1 to have shape (784,) but got array with shape (1,)"
     ]
    }
   ],
   "source": [
    "for img in b[0:]:\n",
    "    display_char(revert_showing(img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "# img = img.astype('uint8') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "Prediction: J\n"
     ]
    }
   ],
   "source": [
    "pred = model.predict(np.array([img]))\n",
    "print(pred)\n",
    "pred_val = res_mapper.get(pred[0].argmax())\n",
    "print(f\"Prediction: {pred_val}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_char(i):\n",
    "    label = test_db.iloc[i].values[0]\n",
    "    char = res_mapper.get(label)\n",
    "    print(\"=============\")\n",
    "    print(f\"label: {char}\")\n",
    "    \n",
    "    img = test_db.iloc[i].values[1:]\n",
    "    \n",
    "    pred = model.predict(np.array([img]))\n",
    "    pred_val = res_mapper.get(pred[0].argmax())\n",
    "    print(f\"Prediction: {pred_val}\")\n",
    "    \n",
    "    \n",
    "    img = img.astype('uint8') \n",
    "    img = img.reshape((28,28))\n",
    "    img = img.transpose()\n",
    "    display(PIL.Image.fromarray(img))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
