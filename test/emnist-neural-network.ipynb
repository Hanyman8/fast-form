{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/adam/Workspace/personal/unihack/fast-form'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir(\"../\")\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_cell_guid": "f2e9cf0b-4025-4de3-b57b-bae842beef1c",
    "_uuid": "4e52b01454a737738ed560d4e25bf770572d57e2"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "train_db = pd.read_csv(\"training_data/emnist-balanced-train.csv\")\n",
    "test_db  = pd.read_csv(\"training_data/emnist-balanced-test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_cell_guid": "db92487d-656a-4cec-958a-0c0c2864a2e6",
    "_uuid": "6124d2bf78a6b9b237cd4e25c0ae3986f060abbd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_train: (112799, 47)\n",
      "x_train: (112799, 784)\n",
      "WARNING:tensorflow:From /home/adam/.local/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:20: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"de...)`\n",
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:28: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/adam/.local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 101519 samples, validate on 11280 samples\n",
      "Epoch 1/10\n",
      "101519/101519 [==============================] - 9s 93us/step - loss: 1.2407 - accuracy: 0.6589 - val_loss: 0.8499 - val_accuracy: 0.7462\n",
      "Epoch 2/10\n",
      "101519/101519 [==============================] - 10s 96us/step - loss: 0.7157 - accuracy: 0.7868 - val_loss: 0.6585 - val_accuracy: 0.7908\n",
      "Epoch 3/10\n",
      "101519/101519 [==============================] - 8s 83us/step - loss: 0.5633 - accuracy: 0.8247 - val_loss: 0.5617 - val_accuracy: 0.8179\n",
      "Epoch 4/10\n",
      "101519/101519 [==============================] - 10s 100us/step - loss: 0.4867 - accuracy: 0.8445 - val_loss: 0.5280 - val_accuracy: 0.8251\n",
      "Epoch 5/10\n",
      "101519/101519 [==============================] - 8s 80us/step - loss: 0.4345 - accuracy: 0.8568 - val_loss: 0.5101 - val_accuracy: 0.8316\n",
      "Epoch 6/10\n",
      "101519/101519 [==============================] - 8s 80us/step - loss: 0.3969 - accuracy: 0.8672 - val_loss: 0.4897 - val_accuracy: 0.8326\n",
      "Epoch 7/10\n",
      "101519/101519 [==============================] - 9s 84us/step - loss: 0.3658 - accuracy: 0.8764 - val_loss: 0.4782 - val_accuracy: 0.8392\n",
      "Epoch 8/10\n",
      "101519/101519 [==============================] - 8s 82us/step - loss: 0.3416 - accuracy: 0.8821 - val_loss: 0.4737 - val_accuracy: 0.8395\n",
      "Epoch 9/10\n",
      "101519/101519 [==============================] - 8s 83us/step - loss: 0.3169 - accuracy: 0.8913 - val_loss: 0.4677 - val_accuracy: 0.8438\n",
      "Epoch 10/10\n",
      "101519/101519 [==============================] - 8s 83us/step - loss: 0.2986 - accuracy: 0.8959 - val_loss: 0.4635 - val_accuracy: 0.8447\n",
      "y_test: (18799, 47)\n",
      "x_test: (112799, 784)\n",
      "18799/18799 [==============================] - 2s 92us/step\n",
      "[0.47837314857016905, 0.8486089706420898]\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import np_utils\n",
    "from keras.layers import Input, Dense, Dropout\n",
    "from keras.models import Model\n",
    "from keras.models import model_from_json\n",
    "\n",
    "num_classes = 47\n",
    "y_train = train_db.iloc[:,0]\n",
    "y_train = np_utils.to_categorical(y_train, num_classes)\n",
    "print (\"y_train:\", y_train.shape)\n",
    "\n",
    "x_train = train_db.iloc[:,1:]\n",
    "x_train = x_train.astype('float32')\n",
    "x_train /= 255\n",
    "print (\"x_train:\",x_train.shape)\n",
    "\n",
    "inp = Input(shape=(784,))\n",
    "hidden_1 = Dense(1024, activation='relu')(inp)\n",
    "dropout_1 = Dropout(0.2)(hidden_1)\n",
    "out = Dense(num_classes, activation='softmax')(hidden_1) \n",
    "model = Model(input=inp, output=out)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', # using the cross-entropy loss function\n",
    "              optimizer='adam', # using the Adam optimiser\n",
    "              metrics=['accuracy']) # reporting the accuracy\n",
    "\n",
    "model.fit(x_train, y_train, # Train the model using the training set...\n",
    "          batch_size=512, nb_epoch=10,\n",
    "          verbose=1, validation_split=0.1) # ...holding out 10% of the data for validation\n",
    "\n",
    "y_test = test_db.iloc[:,0]\n",
    "y_test = np_utils.to_categorical(y_test, num_classes)\n",
    "print (\"y_test:\", y_test.shape)\n",
    "\n",
    "x_test = test_db.iloc[:,1:]\n",
    "x_test = x_test.astype('float32')\n",
    "x_test /= 255\n",
    "print (\"x_test:\",x_train.shape)\n",
    "\n",
    "print(model.evaluate(x_test, y_test, verbose=1)) # Evaluate the trained model on the test set!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 84.86%\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model save/load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 84.86%\n",
      "Saved model to disk\n",
      "Loaded model from disk\n",
      "accuracy: 84.86%\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    " \n",
    "# serialize model to JSON\n",
    "model_json = model.to_json()\n",
    "with open(\"model_data/model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"model_data/model.h5\")\n",
    "print(\"Saved model to disk\")\n",
    " \n",
    "# later...\n",
    " \n",
    "# load json and create model\n",
    "json_file = open('model_data/model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"model_data/model.h5\")\n",
    "print(\"Loaded model from disk\")\n",
    " \n",
    "# evaluate loaded model on test data\n",
    "loaded_model.compile(loss='categorical_crossentropy', # using the cross-entropy loss function\n",
    "              optimizer='adam', # using the Adam optimiser\n",
    "              metrics=['accuracy']) # reporting the accuracy\n",
    "\n",
    "\n",
    "scores = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"%s: %.2f%%\" % (loaded_model.metrics_names[1], scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import PIL.Image\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '0', 1: '1', 2: '2', 3: '3', 4: '4', 5: '5', 6: '6', 7: '7', 8: '8', 9: '9', 10: 'A', 11: 'B', 12: 'C', 13: 'D', 14: 'E', 15: 'F', 16: 'G', 17: 'H', 18: 'I', 19: 'J', 20: 'K', 21: 'L', 22: 'M', 23: 'N', 24: 'O', 25: 'P', 26: 'Q', 27: 'R', 28: 'S', 29: 'T', 30: 'U', 31: 'V', 32: 'W', 33: 'X', 34: 'Y', 35: 'Z', 36: 'a', 37: 'b', 38: 'd', 39: 'e', 40: 'f', 41: 'g', 42: 'h', 43: 'n', 44: 'q', 45: 'r', 46: 't'}\n"
     ]
    }
   ],
   "source": [
    "with open(\"training_data/emnist-balanced-mapping.txt\") as f:\n",
    "    s = f.read()[:-1] # remove last char \\n\n",
    "a = [l.split(\" \") for l in s.split(\"\\n\")]\n",
    "res_mapper = {int(l[0]): chr(int(l[1])) for l in a}\n",
    "print(res_mapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_char(i):\n",
    "    label = test_db.iloc[i].values[0]\n",
    "    char = res_mapper.get(label)\n",
    "    print(\"=============\")\n",
    "    print(f\"label: {char}\")\n",
    "    \n",
    "    img = test_db.iloc[i].values[1:]\n",
    "    \n",
    "    pred = model.predict(np.array([img]))\n",
    "    pred_val = res_mapper.get(pred[0].argmax())\n",
    "    print(f\"Prediction: {pred_val}\")\n",
    "    \n",
    "    \n",
    "    img = img.astype('uint8') \n",
    "    img = img.reshape((28,28))\n",
    "    img = img.transpose()\n",
    "    display(PIL.Image.fromarray(img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# invert\n",
    "# transpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============\n",
      "label: e\n",
      "Prediction: e\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'PIL' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-76416484ae14>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mdisplay_char\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-12-bd36244f90ef>\u001b[0m in \u001b[0;36mdisplay_char\u001b[0;34m(i)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPIL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'PIL' is not defined"
     ]
    }
   ],
   "source": [
    "for i in range (3):\n",
    "    display_char(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "# with open(\"test.txt\", \"wb\") as fp:   #Pickling\n",
    "#     pickle.dump(l, fp)\n",
    "\n",
    "with open(\"./test/example_forms/julinka_dotaznik/example_letters.pickle\", \"rb\") as fp:   # Unpickling\n",
    "        b = pickle.load(fp)\n",
    "\n",
    "len(b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABAAAAAoCAAAAADSsSV9AAAA5klEQVR4nLWRP0oDcRSEv98viyFgDpAYQYikTiFYaCX+OYGdRcD0qT2DZ5CAkFKwETyAWlkEwU5sg1gYSBMw+llkd90cYF/1GGaYN/OCrE6kBCDAqk9Mwku63l0BxIWZ6qHfAtjhTFWnu9RUTmAJTLZCVUWYqdploBqBJ4DXcWpngw3VDvuqYo9NVZimgMRnvYaPDKhzqntUlmYRmiTwyHoe7pY5Y7hJD9YZDBtcLnIJ8DbhvZIzXKNNFsAI/PAJF4XG7vmGA/4lAtsWJFThiCJjRDMjGAT4Sup566X/9vD897h8F4A/pR6XvvDejA4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=16x40 at 0x7FCF7C0D5850>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "img = b[20]\n",
    "display(PIL.Image.fromarray(img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40, 16)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Dimensions :  (40, 16)\n"
     ]
    }
   ],
   "source": [
    "print('Original Dimensions : ',img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4406779661016949"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = 26/59\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26.0"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "59*x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale = 28/max(img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(59, 34)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(61, 36)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imgc = cv2.copyMakeBorder(img,1,1,1,1,cv2.BORDER_CONSTANT,value=[255,255,255])\n",
    "imgc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Dimensions :  (59, 34)\n",
      "Resized Dimensions :  (28, 25)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAHoAAACGCAAAAAA3YsFaAAACZ0lEQVR4nO3aPWhTURTA8X+C4leX11KrS1Hr0qFICRJooVDoIAp2ERSNiLQWQXRTEaSIpRS/BifFtiCKZPFjEWftYEDFKBW0FEuGkpBoTY0IFTTXwaZN6kN855r3sJwz3XPIzY/zknteEhIyBBXhwGSllVZaaaWVVlpppZVWWmmllVZa6WVPt30MiN4TThQDooej5AKiz4/T4tJ27kP16cNbXMuj/Zmq0+fG3evXB9LVpnedqHWpvnnKtWS16Z6rG1yqiUI7oWrTXM5wdGktebvwRQDDCk+Pjtbkh28sqTW0ZGNEJLbxFC+d3zY8aTrj7TlK4XGQtq5ke2Xl+b73ko6R3D6mKtO5zKF+v+jKeNsRjg/Ktnp7m8HGXDHbsJj+yNJ9XyZ77vpVeLajLJ3prNkslG0v+GNar0j3er3gHJh7vZgU90pdBF3f2mah2dGcrcjW9/lIkx1aWB6hLiamPQ5SY8wdnIHSGpplQ1QwSIEd5J8tJE5c3rTV4YpQOBUQncKRjjIRXTtRloTWyWnPI6Us0sXGOovtIvpzahNA52zeQpa91mN974DE1y4bWXCuzQRw0hjTxoz8UIvOdSkepHtWWzUtoOuPAXAzFeK0z7Szf34x0nvJZxrg3l2A+MMA6Klfn0t3+k1HRmxEK3pV/fxi2nea3aPw7Ttr1/hPA58OPiLp9m3770N6+7hgpQLSrhubgXaLGyYgmeHGGHMRuiatJrh4hkejHN9q2XRI+H+zsRfdTQHR/yD+n1+GlVZaaaWVXoZ0bBqY7PWyRW+aSiuttNJKK6200korrbTSSiuttNJK/zl+AmEcPVz8l1KAAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=122x134 at 0x7FCF7C0C6650>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import cv2\n",
    " \n",
    "# img = cv2.imread('/home/img/python.png', cv2.IMREAD_UNCHANGED)\n",
    " \n",
    "print('Original Dimensions : ',img.shape)\n",
    "\n",
    "img = cv2.copyMakeBorder(img,37,38,44,44,cv2.BORDER_CONSTANT,value=[255,255,255])\n",
    "\n",
    "scale = 28/max(img.shape)\n",
    "width = int(img.shape[1] * scale)\n",
    "height = int(img.shape[0] * scale)\n",
    "dim = (width, height)\n",
    "# resize image\n",
    "\n",
    "resized = cv2.resize(img, dim, interpolation = cv2.INTER_AREA)\n",
    " \n",
    "print('Resized Dimensions : ',resized.shape)\n",
    " \n",
    "\n",
    "display(PIL.Image.fromarray(img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(59, 34)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2006,)"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = b[23]\n",
    "print(img.shape)\n",
    "img = img.ravel()\n",
    "img = 255-img\n",
    "img.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAADsAAAAiCAAAAADEl/48AAABnklEQVR4nNWSz0uUURSGH6+CMVHSR87QTIuQ1IJIZ2HQQOKmjauicNEqnPoHJIyoVTsXEVEWjEYi0mKIWbSTXA2mSFioaAhB+INEh29mY4uo8W3xmZiL/M7Upmd1z+U8vId7D7w68vBsLk1FbE1uzuoRty7Y1QlJkj9fXE5a1aqjLjoXHEvfYa3V6DvnnMtIklQuD1vjAXjn+yUpe1e65nne/v1Vv5exPECsDmgGoFAyDnCnqF88SaVSteFyt7l3DqCuPahuFF4b3IATmYvbpzQsj5lcTnUDVxoA+PhiKm9xAbjaAHi3Yap3rxyK2qwk9ey5rfmzdPl60NUJY4/nLXHn337e+azxkxYzvriqwQFJ0tempkR4cdr3S3rjRSIvJSVCLCgAzrnnkqQlgL6tzrBx1dEuSVLxR2HjffgpAU53SNL6wkLL6MH9u3fvRtuBPJsfIDNiSyQZb/+icu6BUQPO3Mz3f9LQN5NUA0TTpA4fimW5v2IOrX8mjc9cMnsAHHsqye4G7xxPiun1yoIrwvqTuzn+z6b4P2j8C3fwJzIxo/Az2EiZAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=59x34 at 0x7F88EFB12050>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "img = img.astype('uint8') \n",
    "img = img.reshape((59, 34))\n",
    "img = img.transpose()\n",
    "display(PIL.Image.fromarray(img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected input_1 to have shape (784,) but got array with shape (1892,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-263-b6b7ce5529b3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mpred_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres_mapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Prediction: {pred_val}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1439\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1440\u001b[0m         \u001b[0;31m# Case 2: Symbolic tensors or Numpy array-like.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1441\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_standardize_user_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1442\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstateful\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1443\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    577\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m             exception_prefix='input')\n\u001b[0m\u001b[1;32m    580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    143\u001b[0m                             \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m                             str(data_shape))\n\u001b[0m\u001b[1;32m    146\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking input: expected input_1 to have shape (784,) but got array with shape (1892,)"
     ]
    }
   ],
   "source": [
    "    pred = model.predict(np.array([img]))\n",
    "    pred_val = res_mapper.get(pred[0].argmax())\n",
    "    print(f\"Prediction: {pred_val}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_char(i):\n",
    "    label = test_db.iloc[i].values[0]\n",
    "    char = res_mapper.get(label)\n",
    "    print(\"=============\")\n",
    "    print(f\"label: {char}\")\n",
    "    \n",
    "    img = test_db.iloc[i].values[1:]\n",
    "    \n",
    "    pred = model.predict(np.array([img]))\n",
    "    pred_val = res_mapper.get(pred[0].argmax())\n",
    "    print(f\"Prediction: {pred_val}\")\n",
    "    \n",
    "    \n",
    "    img = img.astype('uint8') \n",
    "    img = img.reshape((28,28))\n",
    "    img = img.transpose()\n",
    "    display(PIL.Image.fromarray(img))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
